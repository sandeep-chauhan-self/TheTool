# Failure Point Cascade & Data Flow Analysis

## Current Data Flow vs. Where It Breaks

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USER SUBMITS: POST /api/stocks/analyze-all-stocks               â”‚
â”‚ Payload: {"symbols": ["20MICRONS.NS"]}                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Route Handler (stocks.py:analyze_all_stocks)            â”‚
â”‚ âœ“ Receives request                                              â”‚
â”‚ âœ“ Validates payload                                             â”‚
â”‚ âœ— FAILURE POINT 1: Generates UUID (different every time!)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Check for Duplicate (New code from fix)                 â”‚
â”‚ âœ“ Queries analysis_jobs for active jobs (last 5 min)            â”‚
â”‚ âœ— FAILURE POINT 2: No FK relationship!                          â”‚
â”‚   - Can't link analysis_results to analysis_jobs                â”‚
â”‚   - So duplicate check doesn't know which results are from job   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Create Job Record (JobStateTransactions.create_job)     â”‚
â”‚ âœ“ Inserts to analysis_jobs table                                â”‚
â”‚ âœ— FAILURE POINT 3: PRIMARY KEY conflict possible                â”‚
â”‚   - If job_id exists (shouldn't happen with UUID, but...)        â”‚
â”‚   - DB returns error                                             â”‚
â”‚   - Caught and returned as 409                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“ (if insert succeeds)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Start Background Thread                                 â”‚
â”‚ âœ“ Creates new thread in thread_tasks.py                         â”‚
â”‚ âœ“ Passes job_id, symbols, capital, use_demo                     â”‚
â”‚ âœ— FAILURE POINT 4: use_demo not tracked to DB!                  â”‚
â”‚   - analysis_source will be lost                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Thread: analyze_stocks_batch()                          â”‚
â”‚ âœ“ Marks job status as 'processing'                              â”‚
â”‚ âœ“ Loops through symbols                                         â”‚
â”‚ âœ— FAILURE POINT 5: No job_id in analysis_results table!         â”‚
â”‚   - Can't link results back to job                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Analyze Each Stock                                      â”‚
â”‚ âœ“ Calls analyze_ticker(ticker)                                  â”‚
â”‚ âœ“ Gets result: {score, verdict, entry, stop, target, ...}       â”‚
â”‚ âœ— FAILURE POINT 6: result dict may not have analysis_source     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 7: Insert Result to DB                                     â”‚
â”‚ INSERT INTO analysis_results                                    â”‚
â”‚   (ticker, symbol, score, verdict, analysis_source, ...)        â”‚
â”‚                                                                  â”‚
â”‚ âœ— FAILURE POINT 7: No UNIQUE constraint!                        â”‚
â”‚   - Same ticker analyzed twice = 2 rows                         â”‚
â”‚   - No way to know which is latest                              â”‚
â”‚                                                                  â”‚
â”‚ âœ— FAILURE POINT 8: Status column always NULL                    â”‚
â”‚   - Can't query by completion status                            â”‚
â”‚                                                                  â”‚
â”‚ âœ— FAILURE POINT 9: raw_data is huge JSON in TEXT                â”‚
â”‚   - Stores entire indicator array (100KB+)                      â”‚
â”‚   - Every query loads entire JSON                               â”‚
â”‚   - Kills performance with many results                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 8: Update Job Progress                                     â”‚
â”‚ UPDATE analysis_jobs SET completed=X, status='processing'       â”‚
â”‚ âœ— FAILURE POINT 10: No per-stock tracking!                      â”‚
â”‚   - Job shows 50/100 complete                                   â”‚
â”‚   - But which 50? Which failed?                                 â”‚
â”‚   - No analysis_jobs_details table                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 9: Complete Job                                            â”‚
â”‚ âœ“ Mark job as 'completed'                                       â”‚
â”‚ âœ— FAILURE POINT 11: Errors array may be empty!                  â”‚
â”‚   - Failed stocks not recorded to analysis_results              â”‚
â”‚   - Just logged to file                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 10: Frontend Queries Results                               â”‚
â”‚                                                                  â”‚
â”‚ Query 1: SELECT * FROM analysis_results WHERE symbol='TCS'      â”‚
â”‚ âœ— FAILURE POINT 12: Returns 5 rows from different dates!        â”‚
â”‚   - No way to know which is latest                              â”‚
â”‚   - No unique constraint                                        â”‚
â”‚   - Has to ORDER BY created_at DESC and LIMIT 1                 â”‚
â”‚                                                                  â”‚
â”‚ Query 2: SELECT * FROM analysis_results WHERE job_id = ?        â”‚
â”‚ âœ— FAILURE POINT 13: job_id column doesn't exist!                â”‚
â”‚   - Can't query results by job                                  â”‚
â”‚   - Can't delete results if job deleted                         â”‚
â”‚                                                                  â”‚
â”‚ Query 3: SELECT * FROM watchlist WHERE symbol='TCS'             â”‚
â”‚   JOIN analysis_results ON ... WHERE status='completed'         â”‚
â”‚ âœ— FAILURE POINT 14: watchlist has no last_job_id!               â”‚
â”‚   - Can't join watchlist to latest analysis                     â”‚
â”‚   - Multiple LEFT JOINs, O(nÂ²) complexity                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Failure Cascade Map

```
First User Click (New Job)
        â†“
    job_id = uuid.uuid4()
        â†“
    create_job_atomic(job_id)
        â”œâ”€ âœ— Race condition on PRIMARY KEY
        â”œâ”€ âœ— DB lock
        â”œâ”€ âœ— Connection pool issue
        â””â”€ â†’ Returns False on ANY error
            â†“
        Return 409 âŒ
            â†“
    Thread NEVER starts âŒ
            â†“
    Job row exists in DB but analysis stuck âŒ

---

Second User Click (Same Symbols, New UUID)
        â†“
    job_id = uuid.uuid4()  (Different from first!)
        â†“
    Check for active job? 
        â”œâ”€ âœ“ Query analysis_jobs (status='processing')
        â”œâ”€ âœ— But NO FK! Can't verify results exist
        â””â”€ Returns existing job OR creates new
            â†“
        If creates new:
            â”œâ”€ Two jobs analyzing same symbols!
            â”œâ”€ analysis_results gets duplicates
            â””â”€ Frontend confused âŒ
        
        If returns existing:
            â”œâ”€ âœ“ Now returns 200 with is_duplicate
            â””â”€ âœ“ No 409 error!

---

Results Accumulation
        â†“
    analyze_ticker() returns result
        â†“
    INSERT INTO analysis_results
        â”œâ”€ âœ— No job_id column!
        â”œâ”€ âœ— No UNIQUE constraint (ticker, date)
        â”œâ”€ âœ— analysis_source maybe NULL
        â”œâ”€ âœ— status column NULL
        â”œâ”€ âœ— raw_data = 100KB JSON
        â””â”€ â†’ Duplicate rows possible âŒ
            â†“
    SELECT FROM analysis_results
        â”œâ”€ Loads entire table
        â”œâ”€ Loads all 100KB JSON for each row
        â””â”€ O(n) complexity, kills DB âŒ

---

Data Orphaning
        â†“
    Job completes, marked 'completed'
        â†“
    2 weeks pass, cleanup task runs:
        DELETE FROM analysis_jobs WHERE completed_at < 2 weeks ago
        â†“
    âœ— analysis_results rows still exist!
    âœ— No FK constraint to prevent
    âœ— Foreign key reference (non-existent job_id) âŒ
            â†“
    100K orphaned rows remain âŒ
            â†“
    Query SELECT * FROM analysis_results
        Includes orphaned rows âŒ
        Can't trace them to job âŒ
        Can't know if they're valid âŒ
```

---

## Query Performance Degradation Over Time

```
Data Growth:
â”œâ”€ Day 1:     100 results â†’ Query: 50ms âœ“
â”œâ”€ Week 1:  1,000 results â†’ Query: 200ms âœ“
â”œâ”€ Week 2:  2,000 results â†’ Query: 600ms âš ï¸
â”œâ”€ Week 3:  5,000 results â†’ Query: 2000ms âš ï¸
â”œâ”€ Month 1: 10,000 results â†’ Query: 8000ms ğŸ”´
â””â”€ Month 2: 20,000 results â†’ Query timeout ğŸ”´

Why?
1. No index on (symbol, created_at)
2. raw_data column: 20,000 Ã— 100KB = 2GB
3. SELECT * loads entire 2GB
4. No WHERE clause optimization
5. Linear scan of all rows
```

---

## Three Table Design: Sufficient? Analysis

```
Current 3 Tables:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   watchlist     â”‚     â”‚ analysis_results     â”‚     â”‚ analysis_jobs    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)         â”‚     â”‚ id (PK)              â”‚     â”‚ job_id (PK)      â”‚
â”‚ symbol (UNIQUE) â”‚     â”‚ ticker               â”‚     â”‚ status           â”‚
â”‚ name            â”‚     â”‚ symbol               â”‚     â”‚ total            â”‚
â”‚ user_id         â”‚     â”‚ score, verdict       â”‚     â”‚ completed        â”‚
â”‚ created_at      â”‚     â”‚ entry, stop, target  â”‚     â”‚ errors           â”‚
â”‚                 â”‚     â”‚ raw_data (TEXT)      â”‚     â”‚ created_at       â”‚
â”‚                 â”‚     â”‚ analysis_source      â”‚     â”‚ started_at       â”‚
â”‚                 â”‚     â”‚ status (NULL!)       â”‚     â”‚ completed_at     â”‚
â”‚                 â”‚     â”‚ created_at           â”‚     â”‚                  â”‚
â”‚                 â”‚     â”‚ updated_at           â”‚     â”‚                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“                       â†“                             â†“
    Missing:               Missing:                     Missing:
    â€¢ last_job_id          â€¢ job_id (FK!)               â€¢ description
    â€¢ last_status          â€¢ watchlist_id               â€¢ job_source
    â€¢ last_analysis        â€¢ UNIQUE constraint          â€¢ timeout
    â€¢ FK to results        â€¢ status tracking
                          â€¢ per-stock errors
                          â€¢ started_at
                          â€¢ completed_at

Missing Tables:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analysis_jobs_details        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                      â”‚
â”‚ job_id (FK) â†’ analysis_jobs  â”‚
â”‚ ticker                       â”‚
â”‚ status (processing/failed)   â”‚
â”‚ started_at, completed_at     â”‚
â”‚ error_message                â”‚
â”‚ result_id (FK) â†’ results     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ analysis_raw_data            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id (PK)                      â”‚
â”‚ result_id (FK) â†’ results     â”‚
â”‚ raw_indicators (JSON)        â”‚
â”‚ created_at                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Why 5 Tables Better Than 3?
â”œâ”€ Separates concerns
â”œâ”€ Reduces data duplication
â”œâ”€ Improves query performance
â”œâ”€ Enables per-stock tracking
â”œâ”€ Supports error tracking
â””â”€ Better for auditing
```

---

## Fix Priority

**CRITICAL (Fix Today):**
1. âŒ Duplicate job_id PRIMARY KEY violation
   - Add: Composite key strategy (job_id + timestamp hash)
   - Or: UUID collision handling
   
2. âŒ No UNIQUE constraint on (ticker, date)
   - Add: `CREATE UNIQUE INDEX idx_ticker_date ON analysis_results(ticker, DATE(created_at))`

3. âŒ No job_id in analysis_results
   - Add: `job_id TEXT` column to analysis_results
   - Add: `FOREIGN KEY (job_id) REFERENCES analysis_jobs(job_id)`

**HIGH (Fix This Sprint):**
4. âŒ analysis_source lost
   - Fix: Pass use_demo through function calls
   - Populate analysis_source on INSERT

5. âŒ Status column unused
   - Fix: Populate status on INSERT
   - Use in queries for filtering

6. âŒ raw_data performance issue
   - Plan: Create analysis_raw_data table
   - Move 100KB+ JSON to separate table

**MEDIUM (Fix Next Sprint):**
7. âš ï¸ error_message not captured
   - Fix: Try/except around analyze_ticker
   - Insert error rows to analysis_results OR analysis_jobs_details

8. âš ï¸ watchlist-job relationship
   - Add: last_job_id, last_analyzed_at, analysis_status to watchlist

9. âš ï¸ Per-stock job tracking
   - Create: analysis_jobs_details table
   - Track each stock's status within job

10. âš ï¸ Orphaned data problem
    - Add: ON DELETE CASCADE to FKs
    - Or: Archive table for deleted jobs

---

## Summary: 3 Tables - Verdict

**Can 3 tables work?** âœ“ Technically yes
**Will 3 tables work reliably?** âœ— No, too many issues
**Will it scale?** âœ— Performance collapses after 10K results

**Why the 409 error exists:**
- Root cause: PRIMARY KEY violation on job_id (rare but happens on db locks)
- Symptom: 409 returned immediately
- Result: Thread never starts, job stuck

**Why duplicates aren't caught:**
- No FK linking results to jobs
- No UNIQUE constraint on ticker+date
- Multiple rows for same stock possible
- Can't identify "latest" result reliably

**Path to reliability:**
1. Fix the 3 tables immediately (add constraints)
2. Add analysis_jobs_details for per-stock tracking
3. Split raw_data to separate table
4. Test with 100K+ results
5. Monitor performance
6. Plan multi-user support with proper isolation
